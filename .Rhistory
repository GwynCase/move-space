# Look again.
head(nest.coords)
# Rearrange the nest coordinates a little so they're easier to join with the location data.
nest.coords <- select(telemetry.sites, lat, lon, m_tag, f_tag, year, site, nest) %>%
pivot_longer(!c(lat, lon, year, site, nest), names_to='sex', values_to='id') %>%
drop_na(id) %>% rename(n.lat=lat, n.lon=lon) %>%
mutate(sex=case_when(
sex == 'm_tag' ~ 'm',
sex == 'f_tag' ~ 'f'
))
# Look again.
head(nest.coords)
# Now join to main data frame.
df <- left_join(df, nest.coords, by=c('id', 'year'))
# Fill missing coordinates when bird is at the nest.
df <- df %>% mutate(
lat=case_when(
at.base == 1 & is.na(lat) ~ n.lat,
TRUE ~ lat
),
lon=case_when(
at.base == 1 & is.na(lon) ~ n.lon,
TRUE ~ lon
)
)
library(tidyverse)
library(lubridate)
# Make a list of the files containing "HAR".
file.list <- list.files('../data/raw', pattern='HAR', full.names=TRUE)
file.list
# Here's the function.
read <- function(x) {
read_delim(x, ';')
}
# Use the function to read in all the files in the file list all at once,
# and put all their contents into a single data frame.
df <- lapply(file.list, read)
str(df)
# Drop the seventh item from the data frame.
df[[7]] <- NULL
# Reshape the data frame and tidy up.
df <- bind_rows(df) %>%
mutate(Speed=as.numeric(Speed))
# Read in HAR07 csv.
HAR07 <- read_csv("../data/raw/HAR07_RAW_2018-07-08_2019-05-09.csv")
# Let's take a peek at it.
head(HAR07)
head(df)
# Rename columns for consistency.
HAR07 <- HAR07 %>%
rename(Latitude=Lat,
Longitude=Long,
`Searching time`=SearchT,
`Logger ID`=LoggerID,
Temperature=Temp,
`No GPS - timeout`=NoGPS)
# Drop all 2019 points.
HAR07 <- filter(HAR07, Year != 2019)
# Add to the main dataframe.
df <- bind_rows(df, HAR07)
# Make a single datetime column.
# Note that the timezone recorded by the tags is UTC, so that's specified here.
df$datetime <-
strptime(paste(df$Year, df$Month, df$Day, df$Hour, df$Minute, df$Second),
format='%Y%m%d%H%M%S', tz='UTC')
# Then convert the datetime to the correct timezone.
df$datetime <- with_tz(df$datetime, tzone = "America/Los_Angeles")
# For some things, I find it helpful to have a separate "date" and "time" column.
df$date <- date(df$datetime)
df$time <- format(ymd_hms(df$datetime), '%H:%M%S')
# Make a list of all the extra columns we don't need.
e.cols <- c('Month', 'Day', 'Hour', 'Minute', 'Second',
'Altitude', 'Div up', 'Div down', 'No GPS - diving', 'Diving duration',
'Raw latitude', 'Raw Longitude', 'Decision voltage', 'Milisecond',
'Acc_X', 'Acc_Y', 'Acc_Z')
# Remove them from the data frame.
df <- select(df, !any_of(e.cols))
# Rename the remaining columns in a reasonable, consistent way.
df <- rename(df, id='Logger ID',
lat='Latitude',
lon='Longitude',
speed='Speed',
s.time='Searching time',
volt='Voltage',
temp='Temperature',
no.fix='No GPS - timeout',
at.base='In range',
year='Year',
datetime=datetime)
# Read in table of nest coordinates.
telemetry.sites <- read_csv('../data/processed/telemetry_sites.csv')
# Take a quick look.
head(telemetry.sites)
# Rearrange the nest coordinates a little so they're easier to join with the location data.
nest.coords <- select(telemetry.sites, lat, lon, m_tag, f_tag, year, site, nest) %>%
pivot_longer(!c(lat, lon, year, site, nest), names_to='sex', values_to='id') %>%
drop_na(id) %>% rename(n.lat=lat, n.lon=lon) %>%
mutate(sex=case_when(
sex == 'm_tag' ~ 'm',
sex == 'f_tag' ~ 'f'
))
# Look again.
head(nest.coords)
# Now join to main data frame.
df <- left_join(df, nest.coords, by=c('id', 'year'))
# Fill missing coordinates when bird is at the nest.
df <- df %>% mutate(
lat=case_when(
at.base == 1 & is.na(lat) ~ n.lat,
TRUE ~ lat
),
lon=case_when(
at.base == 1 & is.na(lon) ~ n.lon,
TRUE ~ lon
)
)
# Replace NAs with zeroes for the variables no.fix, at.base, and s.time.
df <- df %>% replace_na(list(no.fix=0, at.base=0, s.time=0))
df %>% group_by(id, datetime) %>% summarize(n())
df %>% group_by(id) %>% slice(3)
df %>% select(datetime)
class(df$datetime)
print(df$datetime)
df$datetime
df$datetime
df %>% group_by(id, date, time) %>% summarize(n=n()) %>% filter(n > 1)
df %>% group_by(id, date, time) %>% summarize(n=n()) %>% filter(n > 1) %>% slice(5)
df %>% group_by(id, date, time) %>% summarize(n=n()) %>% filter(n > 1) %>% slice(3)
df %>% group_by(id, date, time) %>% summarize(n=n()) %>% filter(n > 1) %>% slice(1)
df %>% group_by(id, date, time) %>% summarize(n=n()) %>% filter(n > 1) %>% slice(1)
df %>% filter(id == 'HAR02' & date == '2020-06-13')
df %>% filter(id == 'HAR02' & date == '2020-06-13' & time == '19:0100')
# Look closer at some more duplicate points.
df %>% filter(id == 'HAR03' & date == '2020-06-14' & time == '15:1816')
# Remove any duplicates.
df <- df %>% distinct(id, datetime, .keep_all=TRUE)
head(df)
df %>% filter(is.na(lat))
df %>% filter(is.na(lat)) %>% head()
df %>% group_by(id) %>% filter(is.na(lat)) %>% slice(1)
# Pull out a selection of NA locations.
df %>% group_by(id) %>% filter(is.na(lat)) %>% slice(1)
# Remove any missed points.
df <- df %>% filter(!is.na(lat))
# Finally, remove the nest coordinates again.
df <- df %>% select(!c(n.lat, n.lon))
# Finally, remove the nest coordinates again.
df <- df %>% select(!c(n.lat, n.lon))
View(df)
# Some HAR09 points were taken in Vancouver; remove them.
df <- df %>% filter(id != 'HAR09' | lat >= 50)
# Do the exact same for HAR10.
df <- df %>% filter(id != 'HAR10' | lat >= 50)
# There is an erroneous point for HAR07 in Washington.
df <- df %>% filter(id != 'HAR07' | lat >= 47)
# There is also an erroneous point for HAR02 somewhere in Nunavut.
df <- df %>% filter(id != 'HAR02' | lat <= 52)
df %>% filter(id == 'HAR07')
df %>% filter(id == 'HAR07') %>% distinct(year)
# Convert location points to UTMs.
df <- df %>%
st_as_sf(coords=c('lon', 'lat')) %>%
st_set_crs('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs') %>%
st_transform("+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs")
library(sf)
library(sf)
# Convert location points to UTMs.
df <- df %>%
st_as_sf(coords=c('lon', 'lat')) %>%
st_set_crs('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs') %>%
st_transform("+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs")
head(df)
View(df)
df %>% select(geometry)
head(df)
df.br.sf %>% mutate(xcoord = unlist(map(df.br.sf$geometry,1)),
ycoord = unlist(map(df.br.sf$geometry,2))) %>%
data.frame()
df %>% mutate(xcoord = unlist(map(df.br.sf$geometry,1)),
ycoord = unlist(map(df.br.sf$geometry,2))) %>%
data.frame()
df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame()
df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame() %>% view()
# Make separate xcoord and ycoord columns.
df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame()
# Make separate xcoord and ycoord columns.
df <- df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame()
# Take a look.
head(df)
library(tidyverse)
library(lubridate)
# Make a list of the files containing "HAR".
file.list <- list.files('../data/raw', pattern='HAR', full.names=TRUE)
file.list
# Here's the function.
read <- function(x) {
read_delim(x, ';')
}
# Use the function to read in all the files in the file list all at once,
# and put all their contents into a single data frame.
df <- lapply(file.list, read)
str(df)
# Drop the seventh item from the data frame.
df[[7]] <- NULL
# Reshape the data frame and tidy up.
df <- bind_rows(df) %>%
mutate(Speed=as.numeric(Speed))
# Read in HAR07 csv.
HAR07 <- read_csv("../data/raw/HAR07_RAW_2018-07-08_2019-05-09.csv")
# Let's take a peek at it.
head(HAR07)
head(df)
# Rename columns for consistency.
HAR07 <- HAR07 %>%
rename(Latitude=Lat,
Longitude=Long,
`Searching time`=SearchT,
`Logger ID`=LoggerID,
Temperature=Temp,
`No GPS - timeout`=NoGPS)
# Drop all 2019 points.
HAR07 <- filter(HAR07, Year != 2019)
# Add to the main dataframe.
df <- bind_rows(df, HAR07)
# Make a single datetime column.
# Note that the timezone recorded by the tags is UTC, so that's specified here.
df$datetime <-
strptime(paste(df$Year, df$Month, df$Day, df$Hour, df$Minute, df$Second),
format='%Y%m%d%H%M%S', tz='UTC')
# Then convert the datetime to the correct timezone.
df$datetime <- with_tz(df$datetime, tzone = "America/Los_Angeles")
# For some things, I find it helpful to have a separate "date" and "time" column.
df$date <- date(df$datetime)
df$time <- format(ymd_hms(df$datetime), '%H:%M%S')
# Make a list of all the extra columns we don't need.
e.cols <- c('Month', 'Day', 'Hour', 'Minute', 'Second',
'Altitude', 'Div up', 'Div down', 'No GPS - diving', 'Diving duration',
'Raw latitude', 'Raw Longitude', 'Decision voltage', 'Milisecond',
'Acc_X', 'Acc_Y', 'Acc_Z')
# Remove them from the data frame.
df <- select(df, !any_of(e.cols))
# Rename the remaining columns in a reasonable, consistent way.
df <- rename(df, id='Logger ID',
lat='Latitude',
lon='Longitude',
speed='Speed',
s.time='Searching time',
volt='Voltage',
temp='Temperature',
no.fix='No GPS - timeout',
at.base='In range',
year='Year',
datetime=datetime)
# Read in table of nest coordinates.
telemetry.sites <- read_csv('../data/processed/telemetry_sites.csv')
# Take a quick look.
head(telemetry.sites)
# Rearrange the nest coordinates a little so they're easier to join with the location data.
nest.coords <- select(telemetry.sites, lat, lon, m_tag, f_tag, year, site, nest) %>%
pivot_longer(!c(lat, lon, year, site, nest), names_to='sex', values_to='id') %>%
drop_na(id) %>% rename(n.lat=lat, n.lon=lon) %>%
mutate(sex=case_when(
sex == 'm_tag' ~ 'm',
sex == 'f_tag' ~ 'f'
))
# Look again.
head(nest.coords)
# Now join to main data frame.
df <- left_join(df, nest.coords, by=c('id', 'year'))
# Fill missing coordinates when bird is at the nest.
df <- df %>% mutate(
lat=case_when(
at.base == 1 & is.na(lat) ~ n.lat,
TRUE ~ lat
),
lon=case_when(
at.base == 1 & is.na(lon) ~ n.lon,
TRUE ~ lon
)
)
# Replace NAs with zeroes for the variables no.fix, at.base, and s.time.
df <- df %>% replace_na(list(no.fix=0, at.base=0, s.time=0))
# Pull out the duplicate points to look at them.
df %>% group_by(id, date, time) %>% summarize(n=n()) %>% filter(n > 1) %>% slice(1)
# Look closer at some of the duplicate points.
df %>% filter(id == 'HAR02' & date == '2020-06-13' & time == '19:0100')
# Look closer at some more duplicate points.
df %>% filter(id == 'HAR03' & date == '2020-06-14' & time == '15:1816')
# Remove any duplicates.
df <- df %>% distinct(id, datetime, .keep_all=TRUE)
# Pull out a selection of NA locations.
df %>% group_by(id) %>% filter(is.na(lat)) %>% slice(1)
# Remove any missed points.
df <- df %>% filter(!is.na(lat))
# Finally, remove the nest coordinates again.
df <- df %>% select(!c(n.lat, n.lon))
# Some HAR09 points were taken in Vancouver; remove them.
df <- df %>% filter(id != 'HAR09' | lat >= 50)
# Do the exact same for HAR10.
df <- df %>% filter(id != 'HAR10' | lat >= 50)
# There is an erroneous point for HAR07 in Washington.
df <- df %>% filter(id != 'HAR07' | lat >= 47)
# There is also an erroneous point for HAR02 somewhere in Nunavut.
df <- df %>% filter(id != 'HAR02' | lat <= 52)
# Rename HAR07 points in 2020+ as HAR007.
df <- df %>% mutate(id=case_when(
id == 'HAR07' & year >= 2020 ~ 'HAR007',
TRUE ~ id
))
# The site name for STV also needs to be fixed, since it will have been bound incorrectly.
df <- df %>% mutate(site=case_when(
id == 'HAR007' ~ 'STV',
TRUE ~ site
))
library(sf)
# Use
# install.packages('sf')
# if necessary.
# Convert location points to UTMs.
df <- df %>%
st_as_sf(coords=c('lon', 'lat')) %>%
st_set_crs('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs') %>%
st_transform("+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs")
head(df)
# Make separate xcoord and ycoord columns.
df <- df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame()
# Take a look.
head(df)
# Take a look.
head(df)
# Use the function to read in all the files in the file list all at once,
# and put all their contents into a single data frame.
df <- lapply(file.list, read)
# Take a look.
head(df)
# Examine the structure (str) of the data frame
str(df)
write_csv(df, '../data/processed/20210420_clean_telemetry.csv')
class(df)
df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame() %>% class()
test <- df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame()
library(tidyverse)
library(lubridate)
# Make a list of the files containing "HAR".
file.list <- list.files('../data/raw', pattern='HAR', full.names=TRUE)
file.list
# Here's the function.
read <- function(x) {
read_delim(x, ';')
}
# Use the function to read in all the files in the file list all at once,
# and put all their contents into a single data frame.
df <- lapply(file.list, read)
# Take a look.
head(df)
# Examine the structure (str) of the data frame
str(df)
# Drop the seventh item from the data frame.
df[[7]] <- NULL
# Reshape the data frame and tidy up.
df <- bind_rows(df) %>%
mutate(Speed=as.numeric(Speed))
# Read in HAR07 csv.
HAR07 <- read_csv("../data/raw/HAR07_RAW_2018-07-08_2019-05-09.csv")
# Let's take a peek at it.
head(HAR07)
head(df)
# Rename columns for consistency.
HAR07 <- HAR07 %>%
rename(Latitude=Lat,
Longitude=Long,
`Searching time`=SearchT,
`Logger ID`=LoggerID,
Temperature=Temp,
`No GPS - timeout`=NoGPS)
# Drop all 2019 points.
HAR07 <- filter(HAR07, Year != 2019)
# Add to the main dataframe.
df <- bind_rows(df, HAR07)
# Make a single datetime column.
# Note that the timezone recorded by the tags is UTC, so that's specified here.
df$datetime <-
strptime(paste(df$Year, df$Month, df$Day, df$Hour, df$Minute, df$Second),
format='%Y%m%d%H%M%S', tz='UTC')
# Then convert the datetime to the correct timezone.
df$datetime <- with_tz(df$datetime, tzone = "America/Los_Angeles")
# For some things, I find it helpful to have a separate "date" and "time" column.
df$date <- date(df$datetime)
df$time <- format(ymd_hms(df$datetime), '%H:%M%S')
# Make a list of all the extra columns we don't need.
e.cols <- c('Month', 'Day', 'Hour', 'Minute', 'Second',
'Altitude', 'Div up', 'Div down', 'No GPS - diving', 'Diving duration',
'Raw latitude', 'Raw Longitude', 'Decision voltage', 'Milisecond',
'Acc_X', 'Acc_Y', 'Acc_Z')
# Remove them from the data frame.
df <- select(df, !any_of(e.cols))
# Rename the remaining columns in a reasonable, consistent way.
df <- rename(df, id='Logger ID',
lat='Latitude',
lon='Longitude',
speed='Speed',
s.time='Searching time',
volt='Voltage',
temp='Temperature',
no.fix='No GPS - timeout',
at.base='In range',
year='Year',
datetime=datetime)
# Read in table of nest coordinates.
telemetry.sites <- read_csv('../data/processed/telemetry_sites.csv')
# Take a quick look.
head(telemetry.sites)
# Rearrange the nest coordinates a little so they're easier to join with the location data.
nest.coords <- select(telemetry.sites, lat, lon, m_tag, f_tag, year, site, nest) %>%
pivot_longer(!c(lat, lon, year, site, nest), names_to='sex', values_to='id') %>%
drop_na(id) %>% rename(n.lat=lat, n.lon=lon) %>%
mutate(sex=case_when(
sex == 'm_tag' ~ 'm',
sex == 'f_tag' ~ 'f'
))
# Look again.
head(nest.coords)
# Now join to main data frame.
df <- left_join(df, nest.coords, by=c('id', 'year'))
# Fill missing coordinates when bird is at the nest.
df <- df %>% mutate(
lat=case_when(
at.base == 1 & is.na(lat) ~ n.lat,
TRUE ~ lat
),
lon=case_when(
at.base == 1 & is.na(lon) ~ n.lon,
TRUE ~ lon
)
)
# Replace NAs with zeroes for the variables no.fix, at.base, and s.time.
df <- df %>% replace_na(list(no.fix=0, at.base=0, s.time=0))
# Pull out the duplicate points to look at them.
df %>% group_by(id, date, time) %>% summarize(n=n()) %>% filter(n > 1) %>% slice(1)
# Look closer at some of the duplicate points.
df %>% filter(id == 'HAR02' & date == '2020-06-13' & time == '19:0100')
# Look closer at some more duplicate points.
df %>% filter(id == 'HAR03' & date == '2020-06-14' & time == '15:1816')
# Remove any duplicates.
df <- df %>% distinct(id, datetime, .keep_all=TRUE)
# Pull out a selection of NA locations.
df %>% group_by(id) %>% filter(is.na(lat)) %>% slice(1)
# Remove any missed points.
df <- df %>% filter(!is.na(lat))
# Finally, remove the nest coordinates again.
df <- df %>% select(!c(n.lat, n.lon))
# Some HAR09 points were taken in Vancouver; remove them.
df <- df %>% filter(id != 'HAR09' | lat >= 50)
# Do the exact same for HAR10.
df <- df %>% filter(id != 'HAR10' | lat >= 50)
# There is an erroneous point for HAR07 in Washington.
df <- df %>% filter(id != 'HAR07' | lat >= 47)
# There is also an erroneous point for HAR02 somewhere in Nunavut.
df <- df %>% filter(id != 'HAR02' | lat <= 52)
# Rename HAR07 points in 2020+ as HAR007.
df <- df %>% mutate(id=case_when(
id == 'HAR07' & year >= 2020 ~ 'HAR007',
TRUE ~ id
))
# The site name for STV also needs to be fixed, since it will have been bound incorrectly.
df <- df %>% mutate(site=case_when(
id == 'HAR007' ~ 'STV',
TRUE ~ site
))
library(sf)
# Use
# install.packages('sf')
# if necessary.
# Convert location points to UTMs.
df <- df %>%
st_as_sf(coords=c('lon', 'lat')) %>%
st_set_crs('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs') %>%
st_transform("+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs")
head(df)
test <- df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame()
test
test %>% class()
# Make separate xcoord and ycoord columns.
df <- df %>% mutate(xcoord = unlist(map(df$geometry,1)),
ycoord = unlist(map(df$geometry,2))) %>%
data.frame()
class(df)
test %>% st_as_sf(coords=geometry)
test
View(test)
test %>% st_as_sf(coords='geometry')
test$geometry
class(test$geometry)
# Take a look.
head(df)
write_csv(df, '../data/processed/20210420_clean_telemetry.csv')
