# Create selection of hulls based on Boolean
hulls.sel.idx <- which(df1)
full.hulls <- hulls(full.lhs)
selected.hulls <- full.hulls[[1]] [ full.hulls[[1]]@data$pts.idx %in% hulls.sel.idx , ]
# Determine the number of points in training dataset
df.temp <- data.frame(as.numeric(df[,j]))
colnames(df.temp) <- "subset"
total.pts <- sum(df.temp)
# Find middle points of testing data and define as -1
for (i in 2:nrow(df.temp)) {
if (df.temp[i,1] == 0 && df.temp[i-1,1] != 0 && df.temp[i-1,1] != -1) {
df.temp[i+50,1] <- -1
}
}
df.temp <- df.temp[1:nrow(data),]
df.temp <- cbind(coords, df.temp)
# Extract the middle points for testing and record associated coordinates
test.pts <- data.frame()
q = 1
for (i in 1:nrow(df.temp)) {
if (df.temp[i,3] == -1) {
test.pts[q,1] <- df.temp[i,1]
test.pts[q,2] <- df.temp[i,2]
q = q + 1
}
}
colnames(test.pts) <- c("x", "y")
test.pts <- SpatialPoints(test.pts[ , c("x","y")], proj4string=CRS("+proj=utm +south +zone=35 +ellps=WGS84"))
poly <- SpatialPolygons(selected.hulls@polygons, proj4string = CRS('+proj=utm +south +zone=35 +ellps=WGS84'))
# Determine the number of hulls under each test point,
overlay <- data.frame(matrix(0,length(test.pts@coords[,1]),1))
for (i in 1:length(test.pts@coords[,1])) {
overlay.list <- over(test.pts[i], poly, returnList=TRUE)
overlay[i,1] <- length(overlay.list[[1]])
}
hull.mean <- mean(full.lhs[[1]]$hulls@data$area)
#Calculate likelihood
for (i in 1:nrow(overlay)) {
overlay[i,2] <- overlay[i,1]/length(selected.hulls[[1]])
overlay[i,3] <- -log(overlay[i,2])
overlay[i,4] <- log(overlay[i,2],exp(1))
if (overlay[i,1] == 0) {
overlay[i,3] <- -log((1/nrow(data))/100)
overlay[i,4] <- log((1/nrow(data))/100, exp(1))
overlay[i,4] <- log((1/nrow(data))/100, exp(1))
}
}
colnames(overlay) <- c("over", "prob", "loglike", "ln.like")
# Add values to likelihood list
likelihood[[j]] <- as.list(overlay)
}
#Likelihood calculation across subsets, column 3 will represent the AIC equivalent and column 4 is akin to the BIC
log.like <- data.frame(matrix(0,length(likelihood),4))
for (i in 1:length(likelihood)) {
log.like[i,1] <- sum(likelihood[[i]]$loglike, na.rm=TRUE)
log.like[i,2] <- sum(likelihood[[i]]$ln.like, na.rm=TRUE)
log.like[i,3] <- -2*(log.like[i,2]) + 2*k
log.like[i,4] <- -2*(log.like[i,2]) + k*log(as.numeric(count),exp(1))
}
#Sum across each form of likelihood calculation
new.postLike <- sum(log.like[,1])
new.lnLike <- sum(log.like[,2])
new.AIC <- sum(log.like[,3])
new.BIC <- sum(log.like[,4])
s.loops[((k-2)*41)+z,1] <- z
s.loops[((k-2)*41)+z,2] <- current.k_val
s.loops[((k-2)*41)+z,3] <- current.s_val
s.loops[((k-2)*41)+z,4] <- hull.mean
s.loops[((k-2)*41)+z,5] <- new.postLike
s.loops[((k-2)*41)+z,6] <- new.lnLike
s.loops[((k-2)*41)+z,7] <- new.AIC
s.loops[((k-2)*41)+z,8] <- new.BIC
s.loops[((k-2)*41)+z,9] <- as.character(name.list[p])
} # End of s loop
} # End of k loop
print(s.loops)
} # End of parallel p loop
#Create object to store elements made during parallel processing
#(using foreach loop)
track <- foreach(p = 1:length(name.list),
.packages=c("tlocoh", "sp", "maptools", "forecast"),
.errorhandling = 'remove', .combine='rbind') %dopar% {
#Import data from appropriate file path, exluding the actual file name
data <- read.csv('../data/processed/telem_all.csv',
header=TRUE, stringsAsFactors=FALSE) %>%
drop_na('lat')
#Redefine the time signature
data$Datetime <- as.POSIXct(data$date, tz="GMT", origin = "1970-01-01 00:00:00")
full <- data.frame(cbind(data$x, data$y, data$Datetime))
colnames(full) <- c('long', 'lat', 'datetime')
full$datetime <- as.POSIXct(full$datetime, tz="GMT", origin = "1970-01-01 00:00:00")
# Two alternatives here: the first is to remove all NAs entirely from the dataset
#full1 <- full[!is.na(full[,1]),]
# The second is to use a Kalman smoother to fill in the NAs
long <- full$long
z <- long
fit <- auto.arima(long)
kr <- KalmanSmooth(long, fit$model)
id.na <- which(is.na(long))
num <- ncol(kr$smooth)
for (j in id.na) {
z[j] <- kr$smooth[j,num]
}
long <- z
lat <- full$lat
z <- lat
fit <- auto.arima(lat)
kr <- KalmanSmooth(lat, fit$model)
id.na <- which(is.na(lat))
num <- ncol(kr$smooth)
for (j in id.na) {
z[j] <- kr$smooth[j,num]
}
lat <- z
full2 <- cbind(long, lat)
full2 <- cbind(full2, data$Datetime)
colnames(full2) <- c('long', 'lat', 'datetime')
full2 <- as.data.frame(full2)
full2$datetime <- as.POSIXct(full2$datetime, tz="GMT", origin = "1970-01-01 00:00:00")
# Convert to UTM (with the Kalman smoothed dataset), adjust UTM Zone as necessary
full.sp.latlong <- SpatialPoints(full2[ , c("long","lat")], proj4string=CRS("+proj=longlat +ellps=WGS84"))
full.sp.utm <- spTransform(full.sp.latlong, CRS("+proj=utm +south +zone=35 +ellps=WGS84"))
full.mat.utm <- coordinates(full.sp.utm)
colnames(full.mat.utm) <- c("x","y")
full.lxy <- xyt.lxy(xy=full.mat.utm, dt=full$datetime, id=as.character(name.list[p]), proj4string=CRS("+proj=utm +south +zone=35 +ellps=WGS84"), dup.dt.check=FALSE)
# Create 100 training/testing splits
df <- data.frame(matrix(TRUE,nrow(data),100))
for (i in 1:ncol(df)) {
samp <- sample(seq(1,nrow(data),1), round(0.002222*(nrow(data))))
for (j in 1:length(samp)) {
for (k in 1:nrow(df)) {
if (k == samp[j]) {
df[k,i] <- FALSE
}
}
}
}
count <- 0
for (i in 1:ncol(df)) {
new <- table(df[,i])[1]
count <- count + new
}
for (i in 3:nrow(df)) {
for (j in 1:ncol(df)) {
if (df[i-1,j] == FALSE && df[i-2,j] != FALSE) {
for (k in 0:98) {
df[i+k,j] <- FALSE
}
}
}
}
df <- df[1:nrow(data),]
# Create list for output of cluster operation
s.loops <- data.frame(matrix(0,1189,9))
for (k in 2:30) {
for (z in 1:41) {
#Define s and k values to perform grid search
current.s_val <- (z-1) * 0.025
current.k_val <- k
# Calculate the nearest neighbors and create lhs object for full dataset
full.lxy <- lxy.nn.add(full.lxy, k=current.k_val, s=current.s_val, status=F)
full.lhs <- lxy.lhs(full.lxy, k=current.k_val, s=current.s_val, status=F)
coords <- full.lhs[[1]]$pts@coords
# Create list for the negative log likelihood values for each test/train split in df
likelihood <- list()
for (j in 1:ncol(df)) {
# Create a one-column data frame from df
df1 <- df[1:nrow(data),j]
# Create selection of hulls based on Boolean
hulls.sel.idx <- which(df1)
full.hulls <- hulls(full.lhs)
selected.hulls <- full.hulls[[1]] [ full.hulls[[1]]@data$pts.idx %in% hulls.sel.idx , ]
# Determine the number of points in training dataset
df.temp <- data.frame(as.numeric(df[,j]))
colnames(df.temp) <- "subset"
total.pts <- sum(df.temp)
# Find middle points of testing data and define as -1
for (i in 2:nrow(df.temp)) {
if (df.temp[i,1] == 0 && df.temp[i-1,1] != 0 && df.temp[i-1,1] != -1) {
df.temp[i+50,1] <- -1
}
}
df.temp <- df.temp[1:nrow(data),]
df.temp <- cbind(coords, df.temp)
# Extract the middle points for testing and record associated coordinates
test.pts <- data.frame()
q = 1
for (i in 1:nrow(df.temp)) {
if (df.temp[i,3] == -1) {
test.pts[q,1] <- df.temp[i,1]
test.pts[q,2] <- df.temp[i,2]
q = q + 1
}
}
colnames(test.pts) <- c("x", "y")
test.pts <- SpatialPoints(test.pts[ , c("x","y")], proj4string=CRS("+proj=utm +south +zone=35 +ellps=WGS84"))
poly <- SpatialPolygons(selected.hulls@polygons, proj4string = CRS('+proj=utm +south +zone=35 +ellps=WGS84'))
# Determine the number of hulls under each test point,
overlay <- data.frame(matrix(0,length(test.pts@coords[,1]),1))
for (i in 1:length(test.pts@coords[,1])) {
overlay.list <- over(test.pts[i], poly, returnList=TRUE)
overlay[i,1] <- length(overlay.list[[1]])
}
hull.mean <- mean(full.lhs[[1]]$hulls@data$area)
#Calculate likelihood
for (i in 1:nrow(overlay)) {
overlay[i,2] <- overlay[i,1]/length(selected.hulls[[1]])
overlay[i,3] <- -log(overlay[i,2])
overlay[i,4] <- log(overlay[i,2],exp(1))
if (overlay[i,1] == 0) {
overlay[i,3] <- -log((1/nrow(data))/100)
overlay[i,4] <- log((1/nrow(data))/100, exp(1))
overlay[i,4] <- log((1/nrow(data))/100, exp(1))
}
}
colnames(overlay) <- c("over", "prob", "loglike", "ln.like")
# Add values to likelihood list
likelihood[[j]] <- as.list(overlay)
}
#Likelihood calculation across subsets, column 3 will represent the AIC equivalent and column 4 is akin to the BIC
log.like <- data.frame(matrix(0,length(likelihood),4))
for (i in 1:length(likelihood)) {
log.like[i,1] <- sum(likelihood[[i]]$loglike, na.rm=TRUE)
log.like[i,2] <- sum(likelihood[[i]]$ln.like, na.rm=TRUE)
log.like[i,3] <- -2*(log.like[i,2]) + 2*k
log.like[i,4] <- -2*(log.like[i,2]) + k*log(as.numeric(count),exp(1))
}
#Sum across each form of likelihood calculation
new.postLike <- sum(log.like[,1])
new.lnLike <- sum(log.like[,2])
new.AIC <- sum(log.like[,3])
new.BIC <- sum(log.like[,4])
s.loops[((k-2)*41)+z,1] <- z
s.loops[((k-2)*41)+z,2] <- current.k_val
s.loops[((k-2)*41)+z,3] <- current.s_val
s.loops[((k-2)*41)+z,4] <- hull.mean
s.loops[((k-2)*41)+z,5] <- new.postLike
s.loops[((k-2)*41)+z,6] <- new.lnLike
s.loops[((k-2)*41)+z,7] <- new.AIC
s.loops[((k-2)*41)+z,8] <- new.BIC
s.loops[((k-2)*41)+z,9] <- as.character(name.list[p])
} # End of s loop
} # End of k loop
print(s.loops)
} # End of parallel p loop
library(rgdal)
library(sp)
library(maptools)
library(tlocoh)
library(foreach)
library(doParallel)
library(forecast)
#Create a list of .csv file names to import for parallel processing
name.list <- 'telem_all'
# Assign n-1 of the total cores to the cluster operation
num_cores <- detectCores()
cl<-makeCluster(num_cores)
registerDoParallel(cl)
# Load up some libraries.
library('tidyverse')
library('lubridate')
library('sp')
library('sp')
library('rgdal')
# Read in the data.
df <- read.csv('../data/processed/telem_all.csv',
header=TRUE, stringsAsFactors=FALSE) %>%
drop_na('lat')
# Select just one site to work with.
ska <- df %>% filter(site == 'SKA')
# Do the datetime thing.
datetime <- ymd_hms(ska$datetime, tz='America/Vancouver')
# Do the spatial thing. This is a bit different than usual.
ska.sf <- SpatialPoints(ska[ , c('lon', 'lat')],
proj4string=CRS('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs')) %>%
spTransform(CRS('+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs'))
ska.coords <- coordinates(ska.sf)
colnames(ska.coords) <- c('x', 'y')
# Make a lxy object.
ska.lxy <- xyt.lxy(xy=ska.coords, dt=datetime, id='HAR05',
proj4string=CRS('+proj=utm +zone=10
+datum=WGS84 +units=m +no_defs'))
library('tlocoh') # Requires the FNN package to be installed.
# Make a lxy object.
ska.lxy <- xyt.lxy(xy=ska.coords, dt=datetime, id='HAR05',
proj4string=CRS('+proj=utm +zone=10
+datum=WGS84 +units=m +no_defs'))
# See if something jumps out.
lxy.plot.sfinder(ska.lxy)
# See if something jumps out.
lxy.plot.sfinder(ska.lxy, delta.t=3600*c(12,24,36,48,54,60))
lxy.plot.sfinder(ska.lxy, delta.t=3600*c(12,24,36,48,54,60), return=TRUE)
s.plot <- lxy.plot.sfinder(ska.lxy, delta.t=3600*c(12,24,36,48,54,60))
svals.24 <- s.plot[[1]]$svals[["86400"]]
svals.12 <- s.plot[[1]]$svals[['43200']]
median(c(svals.12, svals.24))
median(list(svals.12, svals.24))
median(svals.12)
median(svals.24)
ska.lxy <- lxy.nn.add(ska.lxy, s=0.007, a=auto.a(nnn=18, ptp=0.98))
summary(ska.lxy)
ska.lxy <- lxy.nn.add(ska.lxy, s=0.007, a=60000)
6*10000
2:6*10000
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, a=2:6*10000, iso.add=T)
# Plot isopleths and edge:area curves.
lhs.plot.isoarea(ska.lhs.amixed)
lhs.plot.isoear(ska.lhs.amixed)
lhs.plot.isoarea(ska.lhs.amixed)
lhs.plot.isoear(ska.lhs.amixed)
ska.lhs.amixed <- lhs.iso.add(ska.lhs.amixed)
# create a hullset object using the desired a values.
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, a=2:6*10000, iso.add=T)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
ska.lxy <- lxy.nn.add(ska.lxy, s=0.007, a=auto.a(nnn=18, ptp=0.98))
summary(ska.lxy)
4*10000
# Identify nearest neighbor set large enough to include desired a values.
ska.lxy <- lxy.nn.add(ska.lxy, s=0.007, k=20)
# create a hullset object using the desired a values.
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, k=15:20, iso.add=T)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
# Load up some libraries.
library('tidyverse')
library('lubridate')
library('sp')
library('rgdal')
library('tlocoh') # Requires the FNN package to be installed.
# Read in the data.
df <- read.csv('../data/processed/telem_all.csv',
header=TRUE, stringsAsFactors=FALSE) %>%
drop_na('lat')
# Select just one site to work with.
ska <- df %>% filter(site == 'SKA')
# To make a lxy object, we need datetime and coordinates.
# Do the datetime thing.
datetime <- ymd_hms(ska$datetime, tz='America/Vancouver')
# Do the spatial thing. This is a bit different than usual.
ska.sf <- SpatialPoints(ska[ , c('lon', 'lat')],
proj4string=CRS('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs')) %>%
spTransform(CRS('+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs'))
ska.coords <- coordinates(ska.sf)
colnames(ska.coords) <- c('x', 'y')
# Make a lxy object.
ska.lxy <- xyt.lxy(xy=ska.coords, dt=datetime, id='HAR05',
proj4string=CRS('+proj=utm +zone=10
+datum=WGS84 +units=m +no_defs'))
# See if something jumps out.
lxy.plot.sfinder(ska.lxy, delta.t=3600*c(12,24,36,48,54,60))
s.plot <- lxy.plot.sfinder(ska.lxy, delta.t=3600*c(12,24,36,48,54,60))
svals.24 <- s.plot[[1]]$svals[['86400']]
svals.12 <- s.plot[[1]]$svals[['43200']]
median(svals.12)
median(svals.24)
ska.lxy <- lxy.nn.add(ska.lxy, s=0.007, a=auto.a(nnn=18, ptp=0.98))
summary(ska.lxy)
# Identify nearest neighbor set large enough to include desired a values.
ska.lxy <- lxy.nn.add(ska.lxy, s=0.007, k=20)
# create a hullset object using the desired a values.
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, k=15:20, iso.add=T)
# Plot isopleths and edge:area curves.
lhs.plot.isoarea(ska.lhs.amixed)
lhs.plot.isoear(ska.lhs.amixed)
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
# create a hullset object using the desired a values.
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, k=2:6*10000, iso.add=T)
# create a hullset object using the desired a values.
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, a=2:6*10000, iso.add=T)
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
# create a hullset object using the desired a values.
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, a=2:6*1000, iso.add=T)
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
# create a hullset object using the desired a values.
ska.lhs.amixed <- lxy.lhs(ska.lxy, s=0.007, a=5:10*1000, iso.add=T)
plot(ska.lhs.amixed, iso=T, record=T, ufipt=F)
plot(ska.lhs.amixed, iso=T, ufipt=F)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, ufipt=F)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, ufipt=F)
lhs.plot.isoarea(ska.lhs.amixed)
lhs.plot.isoear(ska.lhs.amixed)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, ufipt=F)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, allpts=T, cex.allpts=0.1, ufipt=F)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, allpts=T, cex.allpts=0.1, ufipt=F)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, allpts=T, cex.allpts=0.1, col.allpts="gray30", ufipt=F)
# Plot the isopleths.
plot(ska.lhs.amixed, iso=T, ufipt=F)
ska.lhs.a7000 <- lhs.select(ska.lhs.amixed, a=7000)
lhs.save(ska.lhs.a7000, dir='../data/processed')
# Load the previous lhs object.
ska.lhs <- load('../data/processed/har05.n4671.s0.007.a7000.iso.lhs.01.RData')
plot(ska.lhs, iso=T)
# Apparently doesn't have denisty isopleths saved? Fix that.
ska.lhs <- lhs.iso.add(ska.lhs, status=FALSE)
plot(ska.lhs.a7000, iso=T)
# Load the previous lhs object.
ska.lhs <-
load('../data/processed/har05.n4671.s0.007.a7000.iso.lhs.01.RData')
ska.lhs <- ska.lhs.a7000
plot(ska.lhs, iso=T)
ska.lhs <- lhs.visit.add(ska.lhs, ivg=3600*20)
hist(toni.lhs, lo.margins.set=FALSE, metric="nsv")
hist(ska.lhs, lo.margins.set=FALSE, metric="nsv")
hist(ska.lhs, metric="nsv")
hist(ska.lhs, metric="mnlv")
plot(ska.lhs, hpp=T, hpp.classify='nsv', col.ramp='rainbow')
plot(ska.lhs, hpp=T, hpp.classify='mnlv', col.ramp='rainbow')
plot(ska.lhs, hpp=T, hpp.classify='nsv', col.ramp='rainbow')
plot(ska.lhs, hpp=T, hpp.classify='mnlv', col.ramp='rainbow')
install.packages("tlocoh.dev")
library('tlocoh.dev')
install.packages(“tlocoh”, dependencies=T, repos=“http://R-Forge.R-project.org”)
)
4+3
install.packages(“tlocoh”, dependencies=T, repos=“http://R-Forge.R-project.org”)
update.packages(oldPkgs="tlocoh", dependencies=T, repos="http://R-Forge.R-project.org")
library('tlocoh.dev')
install.packages("tlocoh", dependencies=TRUE, repos=c("http://R-Forge.R-project.org", "http://cran.cnr.berkeley.edu"))
install.packages("tlocoh", dependencies=TRUE, repos=c("http://R-Forge.R-project.org", "http://cran.cnr.berkeley.edu"))
# Load some packages.
library('tlocoh') # Requires the FNN package to be installed.
library('tlocoh.dev')
# Load the previous lhs object.
load('../data/processed/har05.n4671.s0.007.a7000.iso.lhs.01.RData')
ska.lhs <- ska.lhs.a7000
# Load up some libraries.
library('tidyverse')
library('lubridate')
library('sp')
library('rgdal')
library('tlocoh') # Requires the FNN package to be installed.
# Read in the data.
df <- read.csv('../data/processed/telem_all.csv',
header=TRUE, stringsAsFactors=FALSE) %>%
drop_na('lat')
# Select just one site to work with.
ska <- df %>% filter(site == 'SKA')
# To make a lxy object, we need datetime and coordinates.
# Do the datetime thing.
datetime <- ymd_hms(ska$datetime, tz='America/Vancouver')
# Do the spatial thing. This is a bit different than usual.
ska.sf <- SpatialPoints(ska[ , c('lon', 'lat')],
proj4string=CRS('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs')) %>%
spTransform(CRS('+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs'))
ska.coords <- coordinates(ska.sf)
colnames(ska.coords) <- c('x', 'y')
# Make a lxy object.
ska.lxy <- xyt.lxy(xy=ska.coords, dt=datetime, id='HAR05',
proj4string=CRS('+proj=utm +zone=10
+datum=WGS84 +units=m +no_defs'))
# See if something jumps out.
lxy.plot.sfinder(ska.lxy, delta.t=3600*c(12,24,36,48,54,60))
ska.lxy <- lxy.nn.add(ska.lxy, s=0.007, a=7000)
lxy.save(ska.lxy, dir='../data/processed')
ska.tumap1 <- lxy.tumap(ska.lxy, ivg=10*3600, gridtype="square")
install.packages("tlocoh.dev", repos=c("http://R-Forge.R-project.org", "http://cran.cnr.berkeley.edu"))
library('tlocoh.dev')
ska.tumap1 <- lxy.tumap(ska.lxy, ivg=10*3600, gridtype="square")
plot(ska.tumap1, cex.axis=0.8, cex=0.8, legend='topright')
ska.tumap1 <- lxy.tumap(ska.lxy, ivg=10*3600, gridtype='hex')
plot(ska.tumap1, cex.axis=0.8, cex=0.8, legend='topright')
hull.scatter <- lhs.plot.scatter(ska.lhs, x='nsv', y='mnlv', col='spiral', bg='black')
load('../data/processed/HAR05.n4671.2019-06-23.2019-09-04.lxy.01.RData')
ska.lhs <- lhs.visit.add(toni.lhs, ivg=3600*10)
ska.lhs <- lhs.visit.add(ska.lhs, ivg=3600*10)
hull.scatter <- lhs.plot.scatter(ska.lhs, x='nsv', y='mnlv', col='spiral', bg='black')
ska.lhs <- lhs.ellipses.add(ska.lhs)
# Load some packages.
library('tlocoh') # Requires the FNN package to be installed.
library('tlocoh.dev')
# Load the previous lhs object.
load('../data/processed/har05.n4671.s0.007.a7000.iso.lhs.01.RData')
ska.lhs <- ska.lhs.a7000
load('../data/processed/HAR05.n4671.2019-06-23.2019-09-04.lxy.01.RData')
plot(ska.lhs, iso=T)
ska.lhs <- lhs.visit.add(ska.lhs, ivg=3600*20)
hist(ska.lhs, metric='nsv')
hist(ska.lhs, metric='mnlv')
ska.tumap1 <- lxy.tumap(ska.lxy, ivg=10*3600, gridtype='hex')
plot(ska.tumap1, cex.axis=0.8, cex=0.8, legend='topright')
ska.lhs <- lhs.visit.add(ska.lhs, ivg=3600*10)
hull.scatter <- lhs.plot.scatter(ska.lhs, x='nsv', y='mnlv', col='spiral', bg='black')
ska.lhs. <- lhs.ellipses.add(ska.lhs)
ska.lhs <- ska.lhs.
summary(ska.lhs)
# Generate isopleths based on eccentricity of ellipses.
ska.lhs <- lhs.iso.add(ska.lhs, sort.metric="ecc")
plot(ska.lhs, iso=T, iso.sort.metric="ecc")
# And plot.
plot(ska.lhs, iso=T, iso.sort.metric="ecc")
# And plot.
plot(ska.lhs, iso=T, iso.sort.metric="ecc")
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, messge=FALSE)
ska.tumap1 <- lxy.tumap(ska.lxy, ivg=10*3600, gridtype='hex')
plot(ska.tumap1, cex.axis=0.8, cex=0.8, legend='topright')
ska.lhs <- lhs.visit.add(ska.lhs, ivg=3600*10)
hull.scatter <- lhs.plot.scatter(ska.lhs, x='nsv', y='mnlv', col='spiral', bg='black')
ska.lhs <- lhs.visit.add(ska.lhs, ivg=3600*20)
hist(ska.lhs, metric='nsv')
# And plot.
plot(ska.lhs, iso=T, iso.sort.metric="ecc")
